{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1927c059-7100-4281-b8cf-fc852e4dff8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\hp\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "C:\\Users\\hp\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "# notebooks/eda_modeling.ipynb\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9a9b0c-b587-41cc-a46e-80ddf2e272dc",
   "metadata": {},
   "source": [
    "## 1. Load and Understand data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "31ade6ab-5b28-45da-9497-63a1dea822f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Load data ===\n",
    "train = pd.read_csv('TASK_2/train_set.csv')\n",
    "test = pd.read_csv('TASK_2/test_set.csv')\n",
    "B_test = pd.read_csv('TASK_2/blinded_test_set.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "54b2c297-0311-4179-8e46-d1968367c5b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Feature_1</th>\n",
       "      <th>Feature_2</th>\n",
       "      <th>Feature_3</th>\n",
       "      <th>Feature_4</th>\n",
       "      <th>Feature_5</th>\n",
       "      <th>Feature_6</th>\n",
       "      <th>Feature_7</th>\n",
       "      <th>Feature_8</th>\n",
       "      <th>Feature_9</th>\n",
       "      <th>...</th>\n",
       "      <th>Feature_3230</th>\n",
       "      <th>Feature_3231</th>\n",
       "      <th>Feature_3232</th>\n",
       "      <th>Feature_3233</th>\n",
       "      <th>Feature_3234</th>\n",
       "      <th>Feature_3235</th>\n",
       "      <th>Feature_3236</th>\n",
       "      <th>Feature_3237</th>\n",
       "      <th>Feature_3238</th>\n",
       "      <th>CLASS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID_1</td>\n",
       "      <td>18281.541667</td>\n",
       "      <td>18432.0</td>\n",
       "      <td>9409.650391</td>\n",
       "      <td>0.514708</td>\n",
       "      <td>0.011300</td>\n",
       "      <td>0.045369</td>\n",
       "      <td>2.803803</td>\n",
       "      <td>0.356658</td>\n",
       "      <td>1.803803</td>\n",
       "      <td>...</td>\n",
       "      <td>382.968383</td>\n",
       "      <td>2214.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>136.625113</td>\n",
       "      <td>0.061710</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.154838</td>\n",
       "      <td>4.174959</td>\n",
       "      <td>0.061710</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID_2</td>\n",
       "      <td>20010.083333</td>\n",
       "      <td>20100.0</td>\n",
       "      <td>8303.049072</td>\n",
       "      <td>0.417707</td>\n",
       "      <td>0.014959</td>\n",
       "      <td>0.080294</td>\n",
       "      <td>2.338398</td>\n",
       "      <td>0.429532</td>\n",
       "      <td>1.338398</td>\n",
       "      <td>...</td>\n",
       "      <td>452.986164</td>\n",
       "      <td>2548.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>232.564022</td>\n",
       "      <td>0.090548</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.934229</td>\n",
       "      <td>3.931950</td>\n",
       "      <td>0.090548</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID_3</td>\n",
       "      <td>27260.125000</td>\n",
       "      <td>27437.0</td>\n",
       "      <td>12189.649414</td>\n",
       "      <td>0.447160</td>\n",
       "      <td>0.011428</td>\n",
       "      <td>0.046402</td>\n",
       "      <td>2.782842</td>\n",
       "      <td>0.359345</td>\n",
       "      <td>1.782842</td>\n",
       "      <td>...</td>\n",
       "      <td>419.781765</td>\n",
       "      <td>3400.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>233.593529</td>\n",
       "      <td>0.068704</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.904807</td>\n",
       "      <td>4.085035</td>\n",
       "      <td>0.068704</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID_4</td>\n",
       "      <td>41938.125000</td>\n",
       "      <td>42138.0</td>\n",
       "      <td>17866.433594</td>\n",
       "      <td>0.426019</td>\n",
       "      <td>0.009908</td>\n",
       "      <td>0.034878</td>\n",
       "      <td>3.060655</td>\n",
       "      <td>0.326727</td>\n",
       "      <td>2.060655</td>\n",
       "      <td>...</td>\n",
       "      <td>439.023968</td>\n",
       "      <td>5424.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>427.429572</td>\n",
       "      <td>0.078803</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.870588</td>\n",
       "      <td>4.011726</td>\n",
       "      <td>0.078803</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID_5</td>\n",
       "      <td>41274.125000</td>\n",
       "      <td>41439.0</td>\n",
       "      <td>14315.041992</td>\n",
       "      <td>0.346828</td>\n",
       "      <td>0.013596</td>\n",
       "      <td>0.065680</td>\n",
       "      <td>2.478506</td>\n",
       "      <td>0.403469</td>\n",
       "      <td>1.478506</td>\n",
       "      <td>...</td>\n",
       "      <td>485.209184</td>\n",
       "      <td>5096.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>726.731554</td>\n",
       "      <td>0.142608</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.846909</td>\n",
       "      <td>3.571352</td>\n",
       "      <td>0.142608</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 3240 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     ID     Feature_1  Feature_2     Feature_3  Feature_4  Feature_5  \\\n",
       "0  ID_1  18281.541667    18432.0   9409.650391   0.514708   0.011300   \n",
       "1  ID_2  20010.083333    20100.0   8303.049072   0.417707   0.014959   \n",
       "2  ID_3  27260.125000    27437.0  12189.649414   0.447160   0.011428   \n",
       "3  ID_4  41938.125000    42138.0  17866.433594   0.426019   0.009908   \n",
       "4  ID_5  41274.125000    41439.0  14315.041992   0.346828   0.013596   \n",
       "\n",
       "   Feature_6  Feature_7  Feature_8  Feature_9  ...  Feature_3230  \\\n",
       "0   0.045369   2.803803   0.356658   1.803803  ...    382.968383   \n",
       "1   0.080294   2.338398   0.429532   1.338398  ...    452.986164   \n",
       "2   0.046402   2.782842   0.359345   1.782842  ...    419.781765   \n",
       "3   0.034878   3.060655   0.326727   2.060655  ...    439.023968   \n",
       "4   0.065680   2.478506   0.403469   1.478506  ...    485.209184   \n",
       "\n",
       "   Feature_3231  Feature_3232  Feature_3233  Feature_3234  Feature_3235  \\\n",
       "0        2214.0           1.0    136.625113      0.061710           0.0   \n",
       "1        2548.5           1.0    232.564022      0.090548           0.0   \n",
       "2        3400.0           1.0    233.593529      0.068704           0.0   \n",
       "3        5424.0           1.0    427.429572      0.078803           0.0   \n",
       "4        5096.0           1.0    726.731554      0.142608           0.0   \n",
       "\n",
       "   Feature_3236  Feature_3237  Feature_3238  CLASS  \n",
       "0     28.154838      4.174959      0.061710      0  \n",
       "1     27.934229      3.931950      0.090548      1  \n",
       "2     27.904807      4.085035      0.068704      1  \n",
       "3     27.870588      4.011726      0.078803      0  \n",
       "4     28.846909      3.571352      0.142608      0  \n",
       "\n",
       "[5 rows x 3240 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6473c253-28d0-4239-a13c-a829ad87d30e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Feature_1</th>\n",
       "      <th>Feature_2</th>\n",
       "      <th>Feature_3</th>\n",
       "      <th>Feature_4</th>\n",
       "      <th>Feature_5</th>\n",
       "      <th>Feature_6</th>\n",
       "      <th>Feature_7</th>\n",
       "      <th>Feature_8</th>\n",
       "      <th>Feature_9</th>\n",
       "      <th>...</th>\n",
       "      <th>Feature_3229</th>\n",
       "      <th>Feature_3230</th>\n",
       "      <th>Feature_3231</th>\n",
       "      <th>Feature_3232</th>\n",
       "      <th>Feature_3233</th>\n",
       "      <th>Feature_3234</th>\n",
       "      <th>Feature_3235</th>\n",
       "      <th>Feature_3236</th>\n",
       "      <th>Feature_3237</th>\n",
       "      <th>Feature_3238</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID_101</td>\n",
       "      <td>13249.250000</td>\n",
       "      <td>13323.0</td>\n",
       "      <td>5322.087891</td>\n",
       "      <td>0.401690</td>\n",
       "      <td>0.019253</td>\n",
       "      <td>0.131701</td>\n",
       "      <td>1.965488</td>\n",
       "      <td>0.508780</td>\n",
       "      <td>0.965488</td>\n",
       "      <td>...</td>\n",
       "      <td>453.349939</td>\n",
       "      <td>453.349939</td>\n",
       "      <td>1646.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>162.029162</td>\n",
       "      <td>0.098438</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.580378</td>\n",
       "      <td>3.888605</td>\n",
       "      <td>0.098438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID_102</td>\n",
       "      <td>60593.666667</td>\n",
       "      <td>60804.0</td>\n",
       "      <td>21327.521484</td>\n",
       "      <td>0.351976</td>\n",
       "      <td>0.010976</td>\n",
       "      <td>0.042804</td>\n",
       "      <td>2.858719</td>\n",
       "      <td>0.349807</td>\n",
       "      <td>1.858719</td>\n",
       "      <td>...</td>\n",
       "      <td>492.250478</td>\n",
       "      <td>492.250478</td>\n",
       "      <td>7853.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>961.759455</td>\n",
       "      <td>0.122470</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.690038</td>\n",
       "      <td>3.695084</td>\n",
       "      <td>0.122470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID_103</td>\n",
       "      <td>51978.833333</td>\n",
       "      <td>52193.0</td>\n",
       "      <td>19574.339844</td>\n",
       "      <td>0.376583</td>\n",
       "      <td>0.010708</td>\n",
       "      <td>0.040742</td>\n",
       "      <td>2.906154</td>\n",
       "      <td>0.344097</td>\n",
       "      <td>1.906154</td>\n",
       "      <td>...</td>\n",
       "      <td>482.387417</td>\n",
       "      <td>482.387417</td>\n",
       "      <td>6644.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>763.046057</td>\n",
       "      <td>0.114847</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.037774</td>\n",
       "      <td>3.804517</td>\n",
       "      <td>0.114847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID_104</td>\n",
       "      <td>47737.416667</td>\n",
       "      <td>47943.0</td>\n",
       "      <td>17247.173828</td>\n",
       "      <td>0.361293</td>\n",
       "      <td>0.011891</td>\n",
       "      <td>0.050236</td>\n",
       "      <td>2.710158</td>\n",
       "      <td>0.368982</td>\n",
       "      <td>1.710158</td>\n",
       "      <td>...</td>\n",
       "      <td>475.620243</td>\n",
       "      <td>475.620243</td>\n",
       "      <td>6017.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>718.741732</td>\n",
       "      <td>0.119452</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.964103</td>\n",
       "      <td>3.699860</td>\n",
       "      <td>0.119452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID_105</td>\n",
       "      <td>33029.458333</td>\n",
       "      <td>33261.0</td>\n",
       "      <td>15901.136719</td>\n",
       "      <td>0.481423</td>\n",
       "      <td>0.009294</td>\n",
       "      <td>0.030688</td>\n",
       "      <td>3.194060</td>\n",
       "      <td>0.313081</td>\n",
       "      <td>2.194060</td>\n",
       "      <td>...</td>\n",
       "      <td>417.949466</td>\n",
       "      <td>417.949466</td>\n",
       "      <td>4116.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>314.568513</td>\n",
       "      <td>0.076426</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.802140</td>\n",
       "      <td>4.078748</td>\n",
       "      <td>0.076426</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 3239 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ID     Feature_1  Feature_2     Feature_3  Feature_4  Feature_5  \\\n",
       "0  ID_101  13249.250000    13323.0   5322.087891   0.401690   0.019253   \n",
       "1  ID_102  60593.666667    60804.0  21327.521484   0.351976   0.010976   \n",
       "2  ID_103  51978.833333    52193.0  19574.339844   0.376583   0.010708   \n",
       "3  ID_104  47737.416667    47943.0  17247.173828   0.361293   0.011891   \n",
       "4  ID_105  33029.458333    33261.0  15901.136719   0.481423   0.009294   \n",
       "\n",
       "   Feature_6  Feature_7  Feature_8  Feature_9  ...  Feature_3229  \\\n",
       "0   0.131701   1.965488   0.508780   0.965488  ...    453.349939   \n",
       "1   0.042804   2.858719   0.349807   1.858719  ...    492.250478   \n",
       "2   0.040742   2.906154   0.344097   1.906154  ...    482.387417   \n",
       "3   0.050236   2.710158   0.368982   1.710158  ...    475.620243   \n",
       "4   0.030688   3.194060   0.313081   2.194060  ...    417.949466   \n",
       "\n",
       "   Feature_3230  Feature_3231  Feature_3232  Feature_3233  Feature_3234  \\\n",
       "0    453.349939        1646.0           1.0    162.029162      0.098438   \n",
       "1    492.250478        7853.0           1.0    961.759455      0.122470   \n",
       "2    482.387417        6644.0           1.0    763.046057      0.114847   \n",
       "3    475.620243        6017.0           1.0    718.741732      0.119452   \n",
       "4    417.949466        4116.0           1.0    314.568513      0.076426   \n",
       "\n",
       "   Feature_3235  Feature_3236  Feature_3237  Feature_3238  \n",
       "0           0.0     30.580378      3.888605      0.098438  \n",
       "1           0.0     26.690038      3.695084      0.122470  \n",
       "2           0.0     30.037774      3.804517      0.114847  \n",
       "3           0.0     27.964103      3.699860      0.119452  \n",
       "4           0.0     31.802140      4.078748      0.076426  \n",
       "\n",
       "[5 rows x 3239 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "fd6e0c56-edeb-43a9-be90-37f414df1e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 315 entries, 0 to 314\n",
      "Columns: 3240 entries, ID to CLASS\n",
      "dtypes: float64(3238), int64(1), object(1)\n",
      "memory usage: 7.8+ MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100 entries, 0 to 99\n",
      "Columns: 3240 entries, ID to CLASS\n",
      "dtypes: float64(3238), int64(1), object(1)\n",
      "memory usage: 2.5+ MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 36 entries, 0 to 35\n",
      "Columns: 3239 entries, ID to Feature_3238\n",
      "dtypes: float64(3238), object(1)\n",
      "memory usage: 911.1+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None, None)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.info(), test.info(), B_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d012496f-a9aa-4465-9193-7c55bcb14ad9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((315, 3240), (100, 3240), (36, 3239))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, test.shape, B_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6870c51c-7ee5-4241-881a-d39b89f68f17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(pandas.core.frame.DataFrame, pandas.core.frame.DataFrame, True)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train), type(test), isinstance(train, pd.DataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c66c3fd2-34b4-472e-bcef-c212694e732d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(108, 121)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check which columns are binary features (only contain 0s and 1s).\n",
    "\n",
    "(train.isin([0, 1]).all()).sum(), (test.isin([0, 1]).all()).sum()\n",
    "#print(train.columns[train.isin([0, 1]).all()]),print(test.columns[test.isin([0, 1]).all()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6aa9555-da8d-4895-af35-d31e28b34fbb",
   "metadata": {},
   "source": [
    "### 2) DATA CLEANING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "249be732-81e1-4374-92ce-c09f7aaad066",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2668, 1127, 276)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a) Drop Duplicates\n",
    "train = train.drop_duplicates()\n",
    "test = test.drop_duplicates()\n",
    "blind_test= B_test.drop_duplicates()\n",
    "\n",
    "# b) Missing values\n",
    "misssing_train=train.isnull().sum().sum()\n",
    "misssing_test=test.isnull().sum().sum()\n",
    "misssing_Btest=B_test.isnull().sum().sum()\n",
    "misssing_train,misssing_test, misssing_Btest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c63befa0-e1f8-43c0-8f89-e9f8a32a2cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Feature_1712', 'Feature_1713', 'Feature_1714', 'Feature_1715',\n",
      "       'Feature_1716', 'Feature_1717', 'Feature_1718', 'Feature_1719',\n",
      "       'Feature_1720', 'Feature_1721', 'Feature_1722', 'Feature_1723',\n",
      "       'Feature_1724', 'Feature_1725', 'Feature_1726', 'Feature_1727',\n",
      "       'Feature_1728', 'Feature_1729', 'Feature_1730', 'Feature_1731',\n",
      "       'Feature_1732', 'Feature_1733', 'Feature_1734'],\n",
      "      dtype='object')\n",
      "Index(['Feature_1712', 'Feature_1713', 'Feature_1714', 'Feature_1715',\n",
      "       'Feature_1716', 'Feature_1717', 'Feature_1718', 'Feature_1719',\n",
      "       'Feature_1720', 'Feature_1721', 'Feature_1722', 'Feature_1723',\n",
      "       'Feature_1724', 'Feature_1725', 'Feature_1726', 'Feature_1727',\n",
      "       'Feature_1728', 'Feature_1729', 'Feature_1730', 'Feature_1731',\n",
      "       'Feature_1732', 'Feature_1733', 'Feature_1734'],\n",
      "      dtype='object')\n",
      "Index(['Feature_1712', 'Feature_1713', 'Feature_1714', 'Feature_1715',\n",
      "       'Feature_1716', 'Feature_1717', 'Feature_1718', 'Feature_1719',\n",
      "       'Feature_1720', 'Feature_1721', 'Feature_1722', 'Feature_1723',\n",
      "       'Feature_1724', 'Feature_1725', 'Feature_1726', 'Feature_1727',\n",
      "       'Feature_1728', 'Feature_1729', 'Feature_1730', 'Feature_1731',\n",
      "       'Feature_1732', 'Feature_1733', 'Feature_1734'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "missing_cols = train.columns[train.isnull().any()]\n",
    "print(missing_cols)\n",
    "\n",
    "missing_test_cols = test.columns[test.isnull().any()]\n",
    "print(missing_test_cols)\n",
    "\n",
    "missing_Btest_cols = blind_test.columns[blind_test.isnull().any()]\n",
    "print(missing_Btest_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0180ee78-d8b8-42ee-a107-46c57bb43212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature_1712    36.825397\n",
      "Feature_1713    36.825397\n",
      "Feature_1714    36.825397\n",
      "Feature_1715    36.825397\n",
      "Feature_1716    36.825397\n",
      "Feature_1717    36.825397\n",
      "Feature_1718    36.825397\n",
      "Feature_1719    36.825397\n",
      "Feature_1720    36.825397\n",
      "Feature_1721    36.825397\n",
      "Feature_1722    36.825397\n",
      "Feature_1723    36.825397\n",
      "Feature_1724    36.825397\n",
      "Feature_1725    36.825397\n",
      "Feature_1726    36.825397\n",
      "Feature_1727    36.825397\n",
      "Feature_1728    36.825397\n",
      "Feature_1729    36.825397\n",
      "Feature_1730    36.825397\n",
      "Feature_1731    36.825397\n",
      "Feature_1732    36.825397\n",
      "Feature_1733    36.825397\n",
      "Feature_1734    36.825397\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(((train.isnull().sum() / len(train)) * 100)[train.isnull().any()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "52e76c75-1829-4d7f-b339-e4a3e8c01ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Correlation of features with missing values vs. CLASS:\n",
      "Feature_1712    0.239340\n",
      "Feature_1725    0.224191\n",
      "Feature_1726    0.218184\n",
      "Feature_1730   -0.196653\n",
      "Feature_1713    0.196653\n",
      "Feature_1724    0.196653\n",
      "Feature_1719    0.189340\n",
      "Feature_1729    0.179577\n",
      "Feature_1731    0.174440\n",
      "Feature_1733    0.174440\n",
      "Feature_1722    0.147537\n",
      "Feature_1728    0.147537\n",
      "Feature_1721    0.116677\n",
      "Feature_1716    0.116677\n",
      "Feature_1727    0.115881\n",
      "Feature_1723   -0.076642\n",
      "Feature_1718    0.076642\n",
      "Feature_1717    0.076642\n",
      "Feature_1714   -0.030802\n",
      "Feature_1715    0.001642\n",
      "Feature_1720         NaN\n",
      "Feature_1732         NaN\n",
      "Feature_1734         NaN\n",
      "Name: CLASS, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# ⚠️ 23 columns have missing values, each with ~36.8% missing.\n",
    "# This is a significant proportion, and must be handled carefully to avoid bias or loss of information\n",
    "# If the 23 columns are not strongly correlated with our target or contain low variance, let us consider dropping them using: train.drop(columns=cols_with_high_missing, inplace=True)\n",
    "\n",
    "\n",
    "# Filter only numerical columns among them (correlation works with numeric data)\n",
    "missing_numeric_cols = train[missing_cols].select_dtypes(include=[np.number]).columns\n",
    "\n",
    "#  Compute correlation with target (drop rows with missing values)\n",
    "correlation_with_target = train[missing_numeric_cols.tolist() + ['CLASS']].dropna().corr()['CLASS'].drop('CLASS')\n",
    "\n",
    "# Sort by absolute correlation strength\n",
    "correlation_with_target = correlation_with_target.reindex(correlation_with_target.abs().sort_values(ascending=False).index)\n",
    "\n",
    "# Display result\n",
    "print(\"📊 Correlation of features with missing values vs. CLASS:\")\n",
    "print(correlation_with_target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b6b7e6-9ea5-46cf-8186-a8bb4939245b",
   "metadata": {},
   "source": [
    "#### The result indicates all features with missing values have only weak correlations with the target class._cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dacaa9a-4633-4d43-a9a9-46b67b7b7496",
   "metadata": {},
   "source": [
    "#### ❌ Dropping all features with missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55a59ed-05f0-4395-870f-1f8a986ccafc",
   "metadata": {},
   "source": [
    "Although some features showed weak correlations with the target (|r| < 0.25), none demonstrated strong or moderate predictive power. Since missing values can introduce noise, bias, or require complex imputation, and due to the large number of features available,\n",
    "we choose to remove all columns with missing data to simplify preprocessing and reduce potential overfitting risk.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1271368e-bdb8-42bc-b9f7-f610db598e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop(columns=missing_cols)\n",
    "test = test.drop(columns=missing_test_cols)\n",
    "blind_test = B_test.drop(columns=missing_Btest_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cef37a2d-793f-41a2-9f41-2a720db1d27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target, and also drop ID since it is an identifier not real features.\n",
    "\n",
    "# --------------------\n",
    "X_train = train.drop(columns=['CLASS', 'ID'])\n",
    "y_train = train['CLASS']\n",
    "\n",
    "X_test = test.drop(columns=['CLASS','ID'])\n",
    "y_test = test['CLASS']\n",
    "\n",
    "X_blind = B_test.drop(columns=['ID'])  # No target assumed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7de2b626-84ea-41c9-a834-7b4fd08b65a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values:\n",
      "0 in train\n",
      "0 in test\n",
      "0 in blind test\n"
     ]
    }
   ],
   "source": [
    "# Again Check for missing values \n",
    "print(\"Missing values:\")\n",
    "print(X_train.isna().sum().sum(), \"in train\")\n",
    "print(X_test.isna().sum().sum(), \"in test\")\n",
    "print(X_blind.isna().sum().sum(), \"in blind test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977edad4-1d10-48fa-b186-16a19faa52d5",
   "metadata": {},
   "source": [
    "### c. Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0432237d-422d-4a6a-96da-ee5cb77f337f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clip outliers (by assuming it is safer than removing rows)\n",
    "# Clip values at 1st and 99th percentiles based on training set\n",
    "\n",
    "q_low = X_train.quantile(0.01)\n",
    "q_high = X_train.quantile(0.99)\n",
    "\n",
    "X_train = X_train.clip(q_low, q_high, axis=1)\n",
    "X_test = X_test.clip(q_low, q_high, axis=1)\n",
    "X_blind = X_blind.clip(q_low, q_high, axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6228939f-647f-4b97-97a4-631d8d7dd753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (315, 3215)\n",
      "Test shape: (100, 3215)\n",
      "Blind test shape: (36, 3215)\n"
     ]
    }
   ],
   "source": [
    "# Standardize (fit only on train)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_blind_scaled = scaler.transform(X_blind)\n",
    "\n",
    "\n",
    "# Output: Scaled data is now ready for modeling\n",
    "\n",
    "print(\"Train shape:\", X_train_scaled.shape)\n",
    "print(\"Test shape:\", X_test_scaled.shape)\n",
    "print(\"Blind test shape:\", X_blind_scaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "fe14db31-cd9a-4483-b3bd-767808ccc14f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📘 Logistic Regression Best Params: {'C': 0.1}\n",
      "Metrics: {'Accuracy': 0.62, 'F1-Score': 0.5, 'Recall (Sensitivity)': 0.4523809523809524, 'Specificity (TNR)': 0.7413793103448276, 'AUROC': 0.6510673234811166}\n",
      "\n",
      "📗 SVM Best Params: {'C': 0.1}\n",
      "Metrics: {'Accuracy': 0.58, 'F1-Score': 0.4878048780487805, 'Recall (Sensitivity)': 0.47619047619047616, 'Specificity (TNR)': 0.6551724137931034, 'AUROC': 0.5952380952380952}\n",
      "\n",
      "📙 Random Forest Best Params: {'max_depth': 10, 'max_features': 'log2', 'n_estimators': 50}\n",
      "Metrics: {'Accuracy': 0.61, 'F1-Score': 0.41791044776119407, 'Recall (Sensitivity)': 0.3333333333333333, 'Specificity (TNR)': 0.8103448275862069, 'AUROC': 0.6613300492610837}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, confusion_matrix, f1_score, recall_score, precision_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "\n",
    "# Helper function to calculate additional metrics\n",
    "def evaluate_model(y_true, y_pred, y_proba):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    \n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)  # Sensitivity (TPR)\n",
    "    specificity = tn / (tn + fp)  # TNR\n",
    "    auc = roc_auc_score(y_true, y_proba)\n",
    "\n",
    "    return {\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"F1-Score\": f1,\n",
    "        \"Recall (Sensitivity)\": recall,\n",
    "        \"Specificity (TNR)\": specificity,\n",
    "        \"AUROC\": auc\n",
    "    }\n",
    "\n",
    "# --------------------------------------\n",
    "# 1. Logistic Regression with GridSearch\n",
    "# --------------------------------------\n",
    "logreg_params = {'C': [0.01, 0.1, 1, 10, 100]}\n",
    "logreg_grid = GridSearchCV(LogisticRegression(max_iter=1000, solver='liblinear'), logreg_params, cv=5)\n",
    "logreg_grid.fit(X_train_scaled, y_train)\n",
    "y_pred_logreg = logreg_grid.predict(X_test_scaled)\n",
    "y_proba_logreg = logreg_grid.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "print(\"\\n📘 Logistic Regression Best Params:\", logreg_grid.best_params_)\n",
    "metrics_logreg = evaluate_model(y_test, y_pred_logreg, y_proba_logreg)\n",
    "print(\"Metrics:\", metrics_logreg)\n",
    "\n",
    "# --------------------------------------\n",
    "# 2. SVM with GridSearch (Linear Kernel)\n",
    "# --------------------------------------\n",
    "svm_params = {'C': [0.01, 0.1, 1, 10, 100]}\n",
    "svm_grid = GridSearchCV(SVC(kernel='linear', probability=True), svm_params, cv=5)\n",
    "svm_grid.fit(X_train_scaled, y_train)\n",
    "y_pred_svm = svm_grid.predict(X_test_scaled)\n",
    "y_proba_svm = svm_grid.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "print(\"\\n📗 SVM Best Params:\", svm_grid.best_params_)\n",
    "metrics_svm = evaluate_model(y_test, y_pred_svm, y_proba_svm)\n",
    "print(\"Metrics:\", metrics_svm)\n",
    "\n",
    "# --------------------------------------\n",
    "# 3. Random Forest with GridSearch\n",
    "# --------------------------------------\n",
    "rf_params = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [5, 10, 20],\n",
    "    'max_features': ['sqrt', 'log2']\n",
    "}\n",
    "rf_grid = GridSearchCV(RandomForestClassifier(random_state=42), rf_params, cv=5)\n",
    "rf_grid.fit(X_train_scaled, y_train)\n",
    "y_pred_rf = rf_grid.predict(X_test_scaled)\n",
    "y_proba_rf = rf_grid.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "print(\"\\n📙 Random Forest Best Params:\", rf_grid.best_params_)\n",
    "metrics_rf = evaluate_model(y_test, y_pred_rf, y_proba_rf)\n",
    "print(\"Metrics:\", metrics_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa3c458-e8ef-4609-8bf8-f49f2848ea5b",
   "metadata": {},
   "source": [
    "## Feature Engineering Techniques:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc47def4-b53b-41af-be6f-88ee76a2d603",
   "metadata": {},
   "source": [
    "### PCA (Principal Component Analysis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4b894b17-5b09-4fbb-bff4-e05f5642e1b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (315, 53)\n",
      "Test shape: (100, 53)\n",
      "Blind test shape: (36, 53)\n"
     ]
    }
   ],
   "source": [
    "#📉 PCA (Principal Component Analysis)\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Fit PCA on training data only\n",
    "pca = PCA(n_components=0.95)  # Retain 95% variance\n",
    "X_pca_train = pca.fit_transform(X_train_scaled)\n",
    "\n",
    "# Apply the same transformation to test and blind data\n",
    "X_pca_test = pca.transform(X_test_scaled)\n",
    "X_pca_blind = pca.transform(X_blind_scaled)\n",
    "\n",
    "\n",
    "print(\"Train shape:\", X_pca_train.shape)\n",
    "print(\"Test shape:\", X_pca_test.shape)\n",
    "print(\"Blind test shape:\", X_pca_blind.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "29befaba-d8e9-4187-8cdd-90ef28c887ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📘 Logistic Regression Best Params: {'C': 0.1}\n",
      "Metrics: {'Accuracy': 0.67, 'F1-Score': 0.5352112676056338, 'Recall (Sensitivity)': 0.4523809523809524, 'Specificity (TNR)': 0.8275862068965517, 'AUROC': 0.6921182266009852}\n",
      "\n",
      "📗 SVM Best Params: {'C': 0.1}\n",
      "Metrics: {'Accuracy': 0.59, 'F1-Score': 0.4383561643835616, 'Recall (Sensitivity)': 0.38095238095238093, 'Specificity (TNR)': 0.7413793103448276, 'AUROC': 0.6621510673234811}\n",
      "\n",
      "📙 Random Forest Best Params: {'max_depth': 5, 'max_features': 'log2', 'n_estimators': 100}\n",
      "Metrics: {'Accuracy': 0.57, 'F1-Score': 0.044444444444444446, 'Recall (Sensitivity)': 0.023809523809523808, 'Specificity (TNR)': 0.9655172413793104, 'AUROC': 0.6198686371100164}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, confusion_matrix, f1_score, recall_score, precision_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "\n",
    "# Helper function to calculate additional metrics\n",
    "def evaluate_model(y_true, y_pred, y_proba):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    \n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)  # Sensitivity (TPR)\n",
    "    specificity = tn / (tn + fp)  # TNR\n",
    "    auc = roc_auc_score(y_true, y_proba)\n",
    "\n",
    "    return {\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"F1-Score\": f1,\n",
    "        \"Recall (Sensitivity)\": recall,\n",
    "        \"Specificity (TNR)\": specificity,\n",
    "        \"AUROC\": auc\n",
    "    }\n",
    "\n",
    "# --------------------------------------\n",
    "# 1. Logistic Regression with GridSearch\n",
    "# --------------------------------------\n",
    "logreg_params = {'C': [0.01, 0.1, 1, 10, 100]}\n",
    "logreg_grid = GridSearchCV(LogisticRegression(max_iter=1000, solver='liblinear'), logreg_params, cv=5)\n",
    "logreg_grid.fit( X_pca_train, y_train)\n",
    "y_pred_logreg = logreg_grid.predict(X_pca_test)\n",
    "y_proba_logreg = logreg_grid.predict_proba( X_pca_test)[:, 1]\n",
    "\n",
    "print(\"\\n📘 Logistic Regression Best Params:\", logreg_grid.best_params_)\n",
    "metrics_logreg = evaluate_model(y_test, y_pred_logreg, y_proba_logreg)\n",
    "print(\"Metrics:\", metrics_logreg)\n",
    "\n",
    "# --------------------------------------\n",
    "# 2. SVM with GridSearch (Linear Kernel)\n",
    "# --------------------------------------\n",
    "svm_params = {'C': [0.01, 0.1, 1, 10, 100]}\n",
    "svm_grid = GridSearchCV(SVC(kernel='linear', probability=True), svm_params, cv=5)\n",
    "svm_grid.fit(X_pca_train, y_train)\n",
    "y_pred_svm = svm_grid.predict(X_pca_test)\n",
    "y_proba_svm = svm_grid.predict_proba(X_pca_test)[:, 1]\n",
    "\n",
    "print(\"\\n📗 SVM Best Params:\", svm_grid.best_params_)\n",
    "metrics_svm = evaluate_model(y_test, y_pred_svm, y_proba_svm)\n",
    "print(\"Metrics:\", metrics_svm)\n",
    "\n",
    "# --------------------------------------\n",
    "# 3. Random Forest with GridSearch\n",
    "# --------------------------------------\n",
    "rf_params = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [5, 10, 20],\n",
    "    'max_features': ['sqrt', 'log2']\n",
    "}\n",
    "rf_grid = GridSearchCV(RandomForestClassifier(random_state=42), rf_params, cv=5)\n",
    "rf_grid.fit(X_pca_train, y_train)\n",
    "y_pred_rf = rf_grid.predict(X_pca_test)\n",
    "y_proba_rf = rf_grid.predict_proba(X_pca_test)[:, 1]\n",
    "\n",
    "print(\"\\n📙 Random Forest Best Params:\", rf_grid.best_params_)\n",
    "metrics_rf = evaluate_model(y_test, y_pred_rf, y_proba_rf)\n",
    "print(\"Metrics:\", metrics_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4cc0444a-a4e3-4bb7-951d-5c3d3eb11d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract ID columns from original test and blind test\n",
    "test_ids = test['ID'].values\n",
    "blind_ids = B_test['ID'].values\n",
    "\n",
    "# --- Predictions for test data ---\n",
    "prob_test = logreg_grid.predict_proba(X_pca_test)  # or X_test_selected, X_pca_test as appropriate\n",
    "df_test_pred = pd.DataFrame(prob_test, columns=['Class_0_Prob', 'Class_1_Prob'])\n",
    "df_test_pred.insert(0, 'ID', test_ids)\n",
    "df_test_pred.to_csv(\"test_predictions.csv\", index=False)\n",
    "\n",
    "# --- Predictions for blind test data ---\n",
    "prob_blind = logreg_grid.predict_proba(X_pca_blind)  # or X_blind_selected, X_pca_blind\n",
    "df_blind_pred = pd.DataFrame(prob_blind, columns=['Class_0_Prob', 'Class_1_Prob'])\n",
    "df_blind_pred.insert(0, 'ID', blind_ids)\n",
    "df_blind_pred.to_csv(\"blind_test_predictions.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6ad7c0-6a6b-46b6-80de-d8f7140d879e",
   "metadata": {},
   "source": [
    "### Feature Agglomeration  and Variance Thresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "46e7a7e3-0f5f-4539-808c-d780680ba374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (315, 100)\n",
      "Test shape: (100, 100)\n",
      "Blind test shape: (36, 100)\n"
     ]
    }
   ],
   "source": [
    "#Feature Agglomeration (Clustering-based): Groups similar features into meta-features.\n",
    "from sklearn.cluster import FeatureAgglomeration\n",
    "agglo = FeatureAgglomeration(n_clusters=100)\n",
    "X_train = agglo.fit_transform(X_train)\n",
    "X_test = agglo.fit_transform(X_test)\n",
    "X_blind = agglo.fit_transform(X_blind)\n",
    "\n",
    "\n",
    "print(\"Train shape:\", X_train.shape)\n",
    "print(\"Test shape:\", X_test.shape)\n",
    "print(\"Blind test shape:\", X_blind.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "da84a8b1-14ba-488b-bd2b-c2089cae8bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (315, 99)\n",
      "Test shape: (100, 99)\n",
      "Blind test shape: (36, 99)\n"
     ]
    }
   ],
   "source": [
    "# Variance Thresholding: Removes low-variance (uninformative) features.\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "sel = VarianceThreshold(threshold=0.01)\n",
    "X_train = sel.fit_transform(X_train)\n",
    "X_test = sel.transform(X_test)\n",
    "X_blind = sel.transform(X_blind)\n",
    "\n",
    "print(\"Train shape:\", X_train.shape)\n",
    "print(\"Test shape:\", X_test.shape)\n",
    "print(\"Blind test shape:\", X_blind.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "be26d0b8-33f1-495f-90d8-e2cf7061a9bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (315, 99)\n",
      "Test shape: (100, 99)\n",
      "Blind test shape: (36, 99)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Standardize (fit only on train)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_blind_scaled = scaler.transform(X_blind)\n",
    "\n",
    "\n",
    "# Output: Scaled data is now ready for modeling\n",
    "\n",
    "print(\"Train shape:\", X_train_scaled.shape)\n",
    "print(\"Test shape:\", X_test_scaled.shape)\n",
    "print(\"Blind test shape:\", X_blind_scaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "260ebfad-cdbf-49d1-beb6-5d9508a29e74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📘 Logistic Regression Best Params: {'C': 0.01}\n",
      "Metrics: {'Accuracy': 0.56, 'F1-Score': 0.5686274509803921, 'Recall (Sensitivity)': 0.6904761904761905, 'Specificity (TNR)': 0.46551724137931033, 'AUROC': 0.59688013136289}\n",
      "\n",
      "📗 SVM Best Params: {'C': 0.01}\n",
      "Metrics: {'Accuracy': 0.53, 'F1-Score': 0.5346534653465347, 'Recall (Sensitivity)': 0.6428571428571429, 'Specificity (TNR)': 0.4482758620689655, 'AUROC': 0.5736863711001642}\n",
      "\n",
      "📙 Random Forest Best Params: {'max_depth': 10, 'max_features': 'sqrt', 'n_estimators': 50}\n",
      "Metrics: {'Accuracy': 0.42, 'F1-Score': 0.5857142857142857, 'Recall (Sensitivity)': 0.9761904761904762, 'Specificity (TNR)': 0.017241379310344827, 'AUROC': 0.5045155993431856}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, confusion_matrix, f1_score, recall_score, precision_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "\n",
    "# Helper function to calculate additional metrics\n",
    "def evaluate_model(y_true, y_pred, y_proba):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    \n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)  # Sensitivity (TPR)\n",
    "    specificity = tn / (tn + fp)  # TNR\n",
    "    auc = roc_auc_score(y_true, y_proba)\n",
    "\n",
    "    return {\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"F1-Score\": f1,\n",
    "        \"Recall (Sensitivity)\": recall,\n",
    "        \"Specificity (TNR)\": specificity,\n",
    "        \"AUROC\": auc\n",
    "    }\n",
    "\n",
    "# --------------------------------------\n",
    "# 1. Logistic Regression with GridSearch\n",
    "# --------------------------------------\n",
    "logreg_params = {'C': [0.01, 0.1, 1, 10, 100]}\n",
    "logreg_grid = GridSearchCV(LogisticRegression(max_iter=1000, solver='liblinear'), logreg_params, cv=5)\n",
    "logreg_grid.fit( X_train_scaled, y_train)\n",
    "y_pred_logreg = logreg_grid.predict(X_test_scaled)\n",
    "y_proba_logreg = logreg_grid.predict_proba( X_test_scaled)[:, 1]\n",
    "\n",
    "print(\"\\n📘 Logistic Regression Best Params:\", logreg_grid.best_params_)\n",
    "metrics_logreg = evaluate_model(y_test, y_pred_logreg, y_proba_logreg)\n",
    "print(\"Metrics:\", metrics_logreg)\n",
    "\n",
    "# --------------------------------------\n",
    "# 2. SVM with GridSearch (Linear Kernel)\n",
    "# --------------------------------------\n",
    "svm_params = {'C': [0.01, 0.1, 1, 10, 100]}\n",
    "svm_grid = GridSearchCV(SVC(kernel='linear', probability=True), svm_params, cv=5)\n",
    "svm_grid.fit(X_train_scaled, y_train)\n",
    "y_pred_svm = svm_grid.predict(X_test_scaled)\n",
    "y_proba_svm = svm_grid.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "print(\"\\n📗 SVM Best Params:\", svm_grid.best_params_)\n",
    "metrics_svm = evaluate_model(y_test, y_pred_svm, y_proba_svm)\n",
    "print(\"Metrics:\", metrics_svm)\n",
    "\n",
    "# --------------------------------------\n",
    "# 3. Random Forest with GridSearch\n",
    "# --------------------------------------\n",
    "rf_params = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [5, 10, 20],\n",
    "    'max_features': ['sqrt', 'log2']\n",
    "}\n",
    "rf_grid = GridSearchCV(RandomForestClassifier(random_state=42), rf_params, cv=5)\n",
    "rf_grid.fit(X_train_scaled, y_train)\n",
    "y_pred_rf = rf_grid.predict(X_test_scaled)\n",
    "y_proba_rf = rf_grid.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "print(\"\\n📙 Random Forest Best Params:\", rf_grid.best_params_)\n",
    "metrics_rf = evaluate_model(y_test, y_pred_rf, y_proba_rf)\n",
    "print(\"Metrics:\", metrics_rf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
